<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/png" href="/static/favicon-32x32.png">
    <link rel="stylesheet" href="https://unpkg.com/mvp.css">
    <link href="{{ url_for('static', path='/style.css') }}" rel="stylesheet">
    <title>Large Language Models Capture-the-Flag (LLM CTF) Competition</title>
</head>
<body>
  <header>
    <nav>
         <h1>Cyber@UCI LLM CTF</h1>
         <ul>
             <li><a href="/">Home</a></li>
             <li><a href="/static/rules.pdf">Rules</a></li>
             <li><a href="/leaderboard/">Leaderboard</a></li>
             <li><a href="/attack/">Attack</a></li>
             <li><a href="/defense/">Defend</a></li>
             <li><a href="/docs">API Docs</a></li>
             <li><a href="/api-key">API Key</a></li>
             <li><a target="_blank" href="https://github.com/FireflyHacker/cyberuci-llm-ctf/issues">Issue tracker</a></li>
             <!-- <li><a href="https://docs.google.com/forms/d/e/1FAIpQLSc5lDXapW76S5yp3VylOpzsiOp8l2NgC-aHieYZiFXdLawvsw/viewform">Register Team</a></li> -->
             {% if not logged_in %}
               <li><a href="/login">Login</a></li>
             {% else %}
               <li><a href="/logout">Logout</a></li>
             {% endif %}
         </ul>
    </nav>
    <h1>Cyber@UCI Large Language Models Capture-the-Flag</h1>
  </header>
    <main>
        <p>The aim of the competition is to find out whether simple prompting and filtering mechanisms can make LLM applications robust to prompt injection and extraction.</p>
        <h2 id="competition-overview">Competition Overview</h2>
        <p>In this competition, participants assume the roles of defenders and attackers:</p>
        <ul>
        <li><strong>Defenders</strong> will craft prompts and filters to instruct an LLM to keep a secret, aiming to prevent its discovery in a conversation.</li>
        <li><strong>Attackers</strong> will design strategies to extract the secret from the LLM, circumventing the defender’s safeguards.</li>
        </ul>
        <h2 id="prizes-and-incentives">Prizes and Incentives</h2>
        <ul>
        <li><strong>Prize Pool</strong>: The top 3 defense teams and top 3 attack teams will receive cash prizes of <strong>$000</strong>, <strong>$00</strong>, and <strong>$0</strong>, for a total of <strong>$0</strong>.</li>
        <li><strong>Presentation</strong>: Winners will be forced to tell Cyber@UCI board members how they did it.</li>
        <li><strong>Recognition</strong>: Chance for you to be regcognised by famous hackers such as: Kamaii, Caraboo, and Fireflyhacker</li>
        </ul>
        <h2 id="important-links">Important Links</h2>
        <ul>
        <li><a href="/static/rules.pdf">Official Rules</a></li>
        <li><a href="/docs">API Documentation</a></li>
        <li><a href="/api-key">API Key</a></li>
        <li><a href="/defense">Defense Interface</a></li>
        <li><a href="/attack">Attack Interface</a></li>
        </ul>
        <h2 id="why-this-competition">Why This Competition?</h2>
        <p>Current large language models (LLMs) cannot yet follow initial instructions reliably, if adversarial users or third parties can later provide input to the model. This is a major obstacle to using LLMs as the core of a user-facing application. There exists a growing toolbox of <a href="https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/">attacks</a> that make LLMs obey the attacker’s instructions, and defenses of varying complexity to counter them.</p>
        <p>Application developers who <em>use</em> LLMs, however, can’t always be expected to apply complex defense mechanisms. We aim to find whether a simple approach exists that can withstand adaptive attacks.</p>
        <h2 id="rules-and-engagement">Rules and Engagement</h2>
        <p>For the complete set of rules, please visit the <a href="/static/rules.pdf">official rules page</a>. Collaboration between teams, including between distinct teams in the Attack and Defense track, is not allowed.</p>
        <p><strong>By using this chat interface and the API, you accept that the interactions with the interface and the API can be used for research purposes, and potentially open-sourced by the competition organizers.</strong></p>
        <h3 id="why-this-setup">Why this setup?</h3>
        <p>The goal of the competition is to find out whether there exists a simple <em>prompting</em> approach on the models tested that can make them robust, or robust enough that simple <em>filtering</em> approaches can patch up the remaining vulnerabilities.</p>
        <p>We see this fundamentally as a <em>security</em> problem: thus the defenders cannot change or adapt their defenses once the Reconnaissance phase begins.</p>
        <p>We depart from the standard security threat model in two ways:</p>
        <ul>
        <li><p>The defender is allowed prompting, LLM post-processing, and arbitrary post-processing in Python.</p></li>
        <li><p>We test whether attackers can break a defense in a query-limited setting once they are ready to attack any given defense. The attack is scored based on the number of interactions and tokens it takes them to break the defense.</p></li>
        </ul>
        <p>Both of these are reasonable tradeoffs to make it easier for participants to find interesting defenses and attacks, and for the organizers to evaluate them.</p>
        <p>We choose a black-box setting similar to the real-world LLM application threat model: the attacker has no white-box access to the defender’s security mechanism. However, they can do a large number of queries during the Reconnaissance phase to find out how any defense behaves.</p>
        <h2 id="models-for-testing">Models for Testing</h2>
        <p>The competition will use llama3.2 for testing.</p>      
        <p>Please note that the interface may be buggy on Safari (the CSS may not load properly). Please use another browser, or reload the page until CSS is correctly loaded.</p>
    </main>
</body>
<!-- <script data-goatcounter="https://llmctf23.goatcounter.com/count" data-goatcounter-settings='{"allow_local": true}' async src="//gc.zgo.at/count.js"></script> -->
